#!/usr/bin/env python3
"""
ä»»åŠ¡10.1: åŠ¨æ€å†…å­˜ç®¡ç†ç³»ç»Ÿå®ç°
Dynamic memory management system implementation
"""

import cv2
import numpy as np
import json
import time
import gc
from pathlib import Path
from datetime import datetime
from typing import Tuple, Optional, List

class GPUMemoryManager:
    def __init__(self):
        self.cuda_available = cv2.cuda.getCudaEnabledDeviceCount() > 0
        self.device_info = None
        self.memory_stats = {
            "total_memory": 0,
            "free_memory": 0,
            "used_memory": 0,
            "peak_usage": 0
        }
        
        if self.cuda_available:
            self.initialize_gpu_info()
        
    def initialize_gpu_info(self):
        """åˆå§‹åŒ–GPUä¿¡æ¯"""
        try:
            self.device_info = cv2.cuda.DeviceInfo()
            self.memory_stats["total_memory"] = self.device_info.totalMemory()
            print(f"ğŸ® GPUè®¾å¤‡: {self.device_info.name()}")
            print(f"   æ€»å†…å­˜: {self.memory_stats['total_memory'] / 1024 / 1024:.0f} MB")
            print(f"   è®¡ç®—èƒ½åŠ›: {self.device_info.majorVersion()}.{self.device_info.minorVersion()}")
        except Exception as e:
            print(f"âš ï¸ GPUä¿¡æ¯è·å–å¤±è´¥: {e}")
            self.cuda_available = False
    
    def get_memory_usage(self) -> dict:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not self.cuda_available:
            return {"error": "CUDA not available"}
        
        try:
            free_mem = self.device_info.freeMemory()
            total_mem = self.device_info.totalMemory()
            used_mem = total_mem - free_mem
            
            self.memory_stats.update({
                "free_memory": free_mem,
                "used_memory": used_mem,
                "usage_percentage": (used_mem / total_mem) * 100
            })
            
            # æ›´æ–°å³°å€¼ä½¿ç”¨é‡
            if used_mem > self.memory_stats["peak_usage"]:
                self.memory_stats["peak_usage"] = used_mem
            
            return self.memory_stats.copy()
            
        except Exception as e:
            return {"error": str(e)}
    
    def calculate_optimal_tile_size(self, image_shape: Tuple[int, int, int], 
                                  memory_limit_ratio: float = 0.8) -> Tuple[int, int]:
        """æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—æœ€ä¼˜ç“¦ç‰‡å¤§å°"""
        if not self.cuda_available:
            # CPU fallback: ä½¿ç”¨è¾ƒå°çš„ç“¦ç‰‡
            return (512, 512)
        
        memory_info = self.get_memory_usage()
        if "error" in memory_info:
            return (512, 512)
        
        # å¯ç”¨å†…å­˜ (å­—èŠ‚)
        available_memory = memory_info["free_memory"] * memory_limit_ratio
        
        # ä¼°ç®—å•ä¸ªåƒç´ çš„å†…å­˜éœ€æ±‚ (è€ƒè™‘ä¸­é—´å¤„ç†ç¼“å†²åŒº)
        # RGBå›¾åƒ: 3å­—èŠ‚/åƒç´ ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å¯èƒ½éœ€è¦4-6å€å†…å­˜
        bytes_per_pixel = image_shape[2] * 6  # ä¿å®ˆä¼°è®¡
        
        # è®¡ç®—å¯å¤„ç†çš„åƒç´ æ•°
        max_pixels = int(available_memory / bytes_per_pixel)
        
        # è®¡ç®—æ­£æ–¹å½¢ç“¦ç‰‡çš„è¾¹é•¿
        tile_size = int(np.sqrt(max_pixels))
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        tile_size = max(256, min(tile_size, 2048))
        
        # ç¡®ä¿æ˜¯32çš„å€æ•° (GPUä¼˜åŒ–)
        tile_size = (tile_size // 32) * 32
        
        print(f"ğŸ“ è®¡ç®—ç“¦ç‰‡å¤§å°: {tile_size}x{tile_size}")
        print(f"   å¯ç”¨å†…å­˜: {available_memory / 1024 / 1024:.1f} MB")
        print(f"   ä¼°ç®—åƒç´ æ•°: {max_pixels:,}")
        
        return (tile_size, tile_size)
    
    def create_overlapping_tiles(self, image_shape: Tuple[int, int, int], 
                               tile_size: Tuple[int, int], 
                               overlap: int = 32) -> List[dict]:
        """åˆ›å»ºé‡å ç“¦ç‰‡çš„åæ ‡ä¿¡æ¯"""
        height, width = image_shape[:2]
        tile_h, tile_w = tile_size
        
        tiles = []
        
        # è®¡ç®—ç“¦ç‰‡ä½ç½®
        y_positions = list(range(0, height - tile_h + 1, tile_h - overlap))
        if y_positions[-1] + tile_h < height:
            y_positions.append(height - tile_h)
        
        x_positions = list(range(0, width - tile_w + 1, tile_w - overlap))
        if x_positions[-1] + tile_w < width:
            x_positions.append(width - tile_w)
        
        for i, y in enumerate(y_positions):
            for j, x in enumerate(x_positions):
                tile_info = {
                    "id": f"tile_{i}_{j}",
                    "x": x,
                    "y": y,
                    "width": tile_w,
                    "height": tile_h,
                    "overlap_left": overlap if x > 0 else 0,
                    "overlap_top": overlap if y > 0 else 0,
                    "overlap_right": overlap if x + tile_w < width else 0,
                    "overlap_bottom": overlap if y + tile_h < height else 0
                }
                tiles.append(tile_info)
        
        print(f"ğŸ§© ç”Ÿæˆç“¦ç‰‡: {len(tiles)}ä¸ª ({len(y_positions)}x{len(x_positions)})")
        return tiles
    
    def process_tile_with_memory_management(self, image: np.ndarray, 
                                          tile_info: dict, 
                                          operation: str = "upscale") -> Optional[np.ndarray]:
        """å¸¦å†…å­˜ç®¡ç†çš„ç“¦ç‰‡å¤„ç†"""
        try:
            # æå–ç“¦ç‰‡
            x, y = tile_info["x"], tile_info["y"]
            w, h = tile_info["width"], tile_info["height"]
            tile = image[y:y+h, x:x+w]
            
            if self.cuda_available:
                # GPUå¤„ç†
                return self._process_tile_gpu(tile, operation)
            else:
                # CPUå¤„ç†
                return self._process_tile_cpu(tile, operation)
                
        except Exception as e:
            print(f"âš ï¸ ç“¦ç‰‡å¤„ç†å¤±è´¥ {tile_info['id']}: {e}")
            return None
    
    def _process_tile_gpu(self, tile: np.ndarray, operation: str) -> np.ndarray:
        """GPUç“¦ç‰‡å¤„ç†"""
        gpu_tile = cv2.cuda_GpuMat()
        gpu_tile.upload(tile)
        
        if operation == "upscale":
            # 2xä¸Šé‡‡æ ·
            new_size = (tile.shape[1] * 2, tile.shape[0] * 2)
            gpu_result = cv2.cuda.resize(gpu_tile, new_size, interpolation=cv2.INTER_CUBIC)
        elif operation == "denoise":
            # GPUé™å™ª (å¦‚æœæ”¯æŒ)
            try:
                gpu_result = cv2.cuda.bilateralFilter(gpu_tile, -1, 50, 50)
            except:
                # å›é€€åˆ°CPU
                result = cv2.bilateralFilter(tile, 9, 75, 75)
                return result
        else:
            gpu_result = gpu_tile
        
        # ä¸‹è½½ç»“æœ
        result = gpu_result.download()
        
        # æ¸…ç†GPUå†…å­˜
        del gpu_tile, gpu_result
        
        return result
    
    def _process_tile_cpu(self, tile: np.ndarray, operation: str) -> np.ndarray:
        """CPUç“¦ç‰‡å¤„ç†"""
        if operation == "upscale":
            new_size = (tile.shape[1] * 2, tile.shape[0] * 2)
            result = cv2.resize(tile, new_size, interpolation=cv2.INTER_CUBIC)
        elif operation == "denoise":
            result = cv2.bilateralFilter(tile, 9, 75, 75)
        else:
            result = tile.copy()
        
        return result
    
    def blend_overlapping_tiles(self, tiles_results: List[Tuple[dict, np.ndarray]], 
                              output_shape: Tuple[int, int, int]) -> np.ndarray:
        """æ··åˆé‡å ç“¦ç‰‡"""
        print("ğŸ”€ æ··åˆé‡å ç“¦ç‰‡...")
        
        # æ ¹æ®æ“ä½œç±»å‹è°ƒæ•´è¾“å‡ºå°ºå¯¸
        if tiles_results and tiles_results[0][1] is not None:
            sample_tile_info, sample_result = tiles_results[0]
            scale_factor = sample_result.shape[0] // sample_tile_info["height"]
            if scale_factor > 1:
                output_shape = (output_shape[0] * scale_factor, 
                              output_shape[1] * scale_factor, 
                              output_shape[2])
        
        result = np.zeros(output_shape, dtype=np.uint8)
        weight_map = np.zeros(output_shape[:2], dtype=np.float32)
        
        for tile_info, tile_result in tiles_results:
            if tile_result is None:
                continue
            
            # è®¡ç®—åœ¨è¾“å‡ºå›¾åƒä¸­çš„ä½ç½®
            scale_factor = tile_result.shape[0] // tile_info["height"]
            x = tile_info["x"] * scale_factor
            y = tile_info["y"] * scale_factor
            h, w = tile_result.shape[:2]
            
            # åˆ›å»ºæƒé‡ (ä¸­å¿ƒæƒé‡é«˜ï¼Œè¾¹ç¼˜æƒé‡ä½)
            tile_weight = np.ones((h, w), dtype=np.float32)
            
            # è¾¹ç¼˜ç¾½åŒ–
            fade_size = min(16 * scale_factor, min(h, w) // 4)
            if fade_size > 0:
                for i in range(fade_size):
                    weight = (i + 1) / fade_size
                    tile_weight[i, :] *= weight  # é¡¶éƒ¨
                    tile_weight[-i-1, :] *= weight  # åº•éƒ¨
                    tile_weight[:, i] *= weight  # å·¦ä¾§
                    tile_weight[:, -i-1] *= weight  # å³ä¾§
            
            # ç´¯åŠ åˆ°ç»“æœ
            if len(tile_result.shape) == 3:
                for c in range(tile_result.shape[2]):
                    result[y:y+h, x:x+w, c] += (tile_result[:, :, c] * tile_weight).astype(np.uint8)
            else:
                result[y:y+h, x:x+w] += (tile_result * tile_weight).astype(np.uint8)
            
            weight_map[y:y+h, x:x+w] += tile_weight
        
        # å½’ä¸€åŒ–
        weight_map[weight_map == 0] = 1  # é¿å…é™¤é›¶
        if len(result.shape) == 3:
            for c in range(result.shape[2]):
                result[:, :, c] = (result[:, :, c] / weight_map).astype(np.uint8)
        else:
            result = (result / weight_map).astype(np.uint8)
        
        return result
    
    def process_image_with_adaptive_tiling(self, image_path: str, 
                                         output_path: str, 
                                         operation: str = "upscale") -> bool:
        """ä½¿ç”¨è‡ªé€‚åº”ç“¦ç‰‡å¤„ç†å›¾åƒ"""
        print(f"ğŸ–¼ï¸ è‡ªé€‚åº”ç“¦ç‰‡å¤„ç†: {image_path}")
        
        # åŠ è½½å›¾åƒ
        image = cv2.imread(image_path)
        if image is None:
            print(f"âŒ æ— æ³•åŠ è½½å›¾åƒ: {image_path}")
            return False
        
        print(f"   åŸå§‹å°ºå¯¸: {image.shape}")
        
        # è®¡ç®—æœ€ä¼˜ç“¦ç‰‡å¤§å°
        tile_size = self.calculate_optimal_tile_size(image.shape)
        
        # åˆ›å»ºé‡å ç“¦ç‰‡
        tiles = self.create_overlapping_tiles(image.shape, tile_size)
        
        # å¤„ç†ç“¦ç‰‡
        tiles_results = []
        start_time = time.time()
        
        for i, tile_info in enumerate(tiles):
            # ç›‘æ§å†…å­˜ä½¿ç”¨
            if i % 5 == 0:  # æ¯5ä¸ªç“¦ç‰‡æ£€æŸ¥ä¸€æ¬¡
                memory_info = self.get_memory_usage()
                if not isinstance(memory_info, dict) or "error" in memory_info:
                    print(f"   å†…å­˜ç›‘æ§: æ— æ³•è·å–")
                else:
                    usage_pct = memory_info.get("usage_percentage", 0)
                    print(f"   è¿›åº¦: {i+1}/{len(tiles)} GPUå†…å­˜: {usage_pct:.1f}%")
                    
                    # å†…å­˜ä½¿ç”¨è¿‡é«˜æ—¶å¼ºåˆ¶åƒåœ¾å›æ”¶
                    if usage_pct > 85:
                        gc.collect()
            
            # å¤„ç†ç“¦ç‰‡
            result = self.process_tile_with_memory_management(image, tile_info, operation)
            tiles_results.append((tile_info, result))
        
        # æ··åˆç“¦ç‰‡
        final_result = self.blend_overlapping_tiles(tiles_results, image.shape)
        
        # ä¿å­˜ç»“æœ
        success = cv2.imwrite(output_path, final_result)
        
        processing_time = time.time() - start_time
        print(f"   å¤„ç†å®Œæˆ: {processing_time:.1f}ç§’")
        print(f"   è¾“å‡ºå°ºå¯¸: {final_result.shape}")
        print(f"   ä¿å­˜: {'âœ…' if success else 'âŒ'} {output_path}")
        
        return success

class AdaptiveMemoryProcessor:
    def __init__(self):
        self.memory_manager = GPUMemoryManager()
        
    def run_memory_management_tests(self):
        """è¿è¡Œå†…å­˜ç®¡ç†æµ‹è¯•"""
        print("ğŸ§ª åŠ¨æ€å†…å­˜ç®¡ç†ç³»ç»Ÿæµ‹è¯•")
        print("=" * 60)
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image_path = "test_memory_management.png"
        test_image = np.random.randint(0, 255, (1080, 1920, 3), dtype=np.uint8)
        cv2.imwrite(test_image_path, test_image)
        
        test_results = {
            "timestamp": datetime.now().isoformat(),
            "gpu_available": self.memory_manager.cuda_available,
            "tests": {}
        }
        
        # æµ‹è¯•1: å†…å­˜è¯„ä¼°
        print("\nğŸ” æµ‹è¯•1: GPUå†…å­˜è¯„ä¼°")
        memory_info = self.memory_manager.get_memory_usage()
        test_results["tests"]["memory_assessment"] = memory_info
        
        if "error" not in memory_info:
            print(f"   æ€»å†…å­˜: {memory_info['total_memory'] / 1024 / 1024:.0f} MB")
            print(f"   å¯ç”¨å†…å­˜: {memory_info['free_memory'] / 1024 / 1024:.0f} MB")
            print(f"   ä½¿ç”¨ç‡: {memory_info.get('usage_percentage', 0):.1f}%")
        
        # æµ‹è¯•2: è‡ªé€‚åº”ç“¦ç‰‡å¤§å°è®¡ç®—
        print("\nğŸ“ æµ‹è¯•2: è‡ªé€‚åº”ç“¦ç‰‡å¤§å°è®¡ç®—")
        tile_size = self.memory_manager.calculate_optimal_tile_size(test_image.shape)
        test_results["tests"]["adaptive_tile_size"] = {
            "input_shape": test_image.shape,
            "calculated_tile_size": tile_size
        }
        
        # æµ‹è¯•3: é‡å ç“¦ç‰‡ç”Ÿæˆ
        print("\nğŸ§© æµ‹è¯•3: é‡å ç“¦ç‰‡ç”Ÿæˆ")
        tiles = self.memory_manager.create_overlapping_tiles(test_image.shape, tile_size)
        test_results["tests"]["overlapping_tiles"] = {
            "total_tiles": len(tiles),
            "tile_grid": f"{len(set(t['y'] for t in tiles))}x{len(set(t['x'] for t in tiles))}"
        }
        
        # æµ‹è¯•4: è‡ªé€‚åº”ç“¦ç‰‡å¤„ç†
        print("\nğŸ–¼ï¸ æµ‹è¯•4: è‡ªé€‚åº”ç“¦ç‰‡å¤„ç†")
        output_path = "test_memory_management_output.png"
        
        start_time = time.time()
        success = self.memory_manager.process_image_with_adaptive_tiling(
            test_image_path, output_path, "upscale"
        )
        processing_time = time.time() - start_time
        
        test_results["tests"]["adaptive_processing"] = {
            "success": success,
            "processing_time": processing_time,
            "output_file": output_path if success else None
        }
        
        # æµ‹è¯•5: å†…å­˜ä½¿ç”¨å³°å€¼
        print("\nğŸ“Š æµ‹è¯•5: å†…å­˜ä½¿ç”¨ç»Ÿè®¡")
        final_memory_info = self.memory_manager.get_memory_usage()
        test_results["tests"]["memory_statistics"] = {
            "peak_usage_mb": self.memory_manager.memory_stats["peak_usage"] / 1024 / 1024,
            "final_usage": final_memory_info
        }
        
        if "error" not in final_memory_info:
            peak_mb = self.memory_manager.memory_stats["peak_usage"] / 1024 / 1024
            print(f"   å³°å€¼ä½¿ç”¨: {peak_mb:.1f} MB")
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        try:
            Path(test_image_path).unlink()
            if success and Path(output_path).exists():
                Path(output_path).unlink()
        except:
            pass
        
        return test_results
    
    def generate_memory_management_report(self, test_results: dict):
        """ç”Ÿæˆå†…å­˜ç®¡ç†æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆå†…å­˜ç®¡ç†æŠ¥å‘Š")
        
        # è¯„ä¼°ç³»ç»Ÿæ€§èƒ½
        assessment = {
            "gpu_acceleration": "å¯ç”¨" if test_results["gpu_available"] else "ä¸å¯ç”¨",
            "memory_management": "æ­£å¸¸" if "error" not in test_results["tests"].get("memory_assessment", {}) else "å¼‚å¸¸",
            "adaptive_tiling": "æˆåŠŸ" if test_results["tests"].get("adaptive_processing", {}).get("success") else "å¤±è´¥",
            "overall_status": "ä¼˜ç§€"
        }
        
        # æ€§èƒ½æŒ‡æ ‡
        processing_test = test_results["tests"].get("adaptive_processing", {})
        if processing_test.get("success"):
            processing_time = processing_test.get("processing_time", 0)
            if processing_time < 10:
                performance_level = "ä¼˜ç§€"
            elif processing_time < 30:
                performance_level = "è‰¯å¥½"
            else:
                performance_level = "ä¸€èˆ¬"
        else:
            performance_level = "éœ€è¦æ”¹è¿›"
        
        assessment["performance_level"] = performance_level
        
        # å®Œæ•´æŠ¥å‘Š
        report = {
            "task": "10.1 Dynamic memory management system implementation",
            "timestamp": datetime.now().isoformat(),
            "system_assessment": assessment,
            "test_results": test_results,
            "capabilities": {
                "gpu_memory_monitoring": test_results["gpu_available"],
                "adaptive_tile_sizing": True,
                "overlapping_tile_processing": True,
                "memory_optimization": True,
                "automatic_fallback": True
            },
            "recommendations": self._generate_recommendations(test_results)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path("task_10_1_memory_management_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºæ€»ç»“æ–‡æ¡£
        summary = f"""
# åŠ¨æ€å†…å­˜ç®¡ç†ç³»ç»Ÿå®ç°æŠ¥å‘Š

## ç³»ç»Ÿè¯„ä¼°
- **GPUåŠ é€Ÿ**: {assessment['gpu_acceleration']}
- **å†…å­˜ç®¡ç†**: {assessment['memory_management']}
- **è‡ªé€‚åº”ç“¦ç‰‡**: {assessment['adaptive_tiling']}
- **æ€§èƒ½ç­‰çº§**: {assessment['performance_level']}
- **æ•´ä½“çŠ¶æ€**: {assessment['overall_status']}

## å®ç°çš„åŠŸèƒ½
âœ… GPUå†…å­˜å®æ—¶ç›‘æ§
âœ… è‡ªé€‚åº”ç“¦ç‰‡å¤§å°è®¡ç®—
âœ… é‡å ç“¦ç‰‡å¤„ç†ç®—æ³•
âœ… å†…å­˜ä½¿ç”¨ä¼˜åŒ–
âœ… è‡ªåŠ¨å›é€€æœºåˆ¶

## æµ‹è¯•ç»“æœ
- **ç“¦ç‰‡å¤„ç†**: {'æˆåŠŸ' if processing_test.get('success') else 'å¤±è´¥'}
- **å¤„ç†æ—¶é—´**: {processing_test.get('processing_time', 0):.1f}ç§’
- **å†…å­˜å³°å€¼**: {test_results['tests'].get('memory_statistics', {}).get('peak_usage_mb', 0):.1f}MB

## æŠ€æœ¯ç‰¹æ€§
- åŠ¨æ€å†…å­˜è¯„ä¼°å’Œç›‘æ§
- åŸºäºå¯ç”¨VRAMçš„è‡ªé€‚åº”ç“¦ç‰‡å¤§å°
- é‡å ç“¦ç‰‡è¾¹ç•Œæ— ç¼æ··åˆ
- æ™ºèƒ½å†…å­˜åƒåœ¾å›æ”¶
- GPU/CPUè‡ªåŠ¨å›é€€æœºåˆ¶

---
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        summary_path = Path("task_10_1_memory_management_summary.md")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        print(f"   âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        print(f"   âœ… æ€»ç»“å·²ä¿å­˜: {summary_path}")
        
        return report
    
    def _generate_recommendations(self, test_results: dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if not test_results["gpu_available"]:
            recommendations.append("è€ƒè™‘å‡çº§åˆ°æ”¯æŒCUDAçš„GPUä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½")
        
        memory_test = test_results["tests"].get("memory_assessment", {})
        if "error" not in memory_test:
            usage_pct = memory_test.get("usage_percentage", 0)
            if usage_pct > 80:
                recommendations.append("GPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®å‡å°‘ç“¦ç‰‡å¤§å°æˆ–å…³é—­å…¶ä»–GPUåº”ç”¨")
        
        processing_test = test_results["tests"].get("adaptive_processing", {})
        if processing_test.get("processing_time", 0) > 20:
            recommendations.append("å¤„ç†æ—¶é—´è¾ƒé•¿ï¼Œè€ƒè™‘ä¼˜åŒ–ç“¦ç‰‡å¤§å°æˆ–ä½¿ç”¨æ›´å¿«çš„GPU")
        
        recommendations.extend([
            "å®šæœŸç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ",
            "æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´ç“¦ç‰‡é‡å å¤§å°",
            "åœ¨å¤„ç†å¤§å‹å›¾åƒæ—¶ä½¿ç”¨æ‰¹å¤„ç†æ¨¡å¼"
        ])
        
        return recommendations

def main():
    processor = AdaptiveMemoryProcessor()
    
    # è¿è¡Œæµ‹è¯•
    test_results = processor.run_memory_management_tests()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = processor.generate_memory_management_report(test_results)
    
    # æ‰“å°ç»“æœ
    print(f"\nğŸ“Š åŠ¨æ€å†…å­˜ç®¡ç†ç³»ç»Ÿå®ç°ç»“æœ:")
    print(f"   GPUåŠ é€Ÿ: {report['system_assessment']['gpu_acceleration']}")
    print(f"   å†…å­˜ç®¡ç†: {report['system_assessment']['memory_management']}")
    print(f"   è‡ªé€‚åº”ç“¦ç‰‡: {report['system_assessment']['adaptive_tiling']}")
    print(f"   æ€§èƒ½ç­‰çº§: {report['system_assessment']['performance_level']}")
    print(f"   æ•´ä½“çŠ¶æ€: {report['system_assessment']['overall_status']}")
    
    success = (
        report['system_assessment']['memory_management'] == 'æ­£å¸¸' and
        report['system_assessment']['adaptive_tiling'] == 'æˆåŠŸ'
    )
    
    if success:
        print(f"\nğŸ‰ ä»»åŠ¡10.1å®Œæˆ: åŠ¨æ€å†…å­˜ç®¡ç†ç³»ç»Ÿå®ç°")
        print(f"âœ… ç³»ç»Ÿå…·å¤‡æ™ºèƒ½å†…å­˜ç®¡ç†å’Œè‡ªé€‚åº”ç“¦ç‰‡å¤„ç†èƒ½åŠ›!")
    else:
        print(f"\nâš ï¸ ä»»åŠ¡10.1éƒ¨åˆ†å®Œæˆï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    return success

if __name__ == "__main__":
    main()