# Huaju4K è¯å‰§æ¯ç‰ˆçº§4Kå¢å¼ºç³»ç»Ÿ - è®¾è®¡æ–‡æ¡£ (å®ç°ç‰ˆ)

## ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLI Interface                            â”‚
â”‚              python -m huaju4k enhance                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              VideoEnhancementProcessor                     â”‚
â”‚                   (ä¸»æ§åˆ¶å™¨)                                â”‚
â”‚         ä½¿ç”¨ StrategyDrivenModelManager                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 1    â”‚ â”‚Stage 2 â”‚ â”‚  Stage 4    â”‚
â”‚ èˆå°ç»“æ„åˆ†æ  â”‚ â”‚ç­–ç•¥ç”Ÿæˆâ”‚ â”‚ ä¸‰é˜¶æ®µå¢å¼º   â”‚
â”‚   (CPU)      â”‚ â”‚ (CPU)  â”‚ â”‚  (FFmpeg)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸‰é˜¶æ®µè§†é¢‘å¢å¼ºæµç¨‹ (Stage 4)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ThreeStageVideoEnhancer                   â”‚
â”‚                    (å…¨æµå¼FFmpegå¤„ç†)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚                 â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 4.1    â”‚ â”‚  Stage 4.2    â”‚ â”‚  Stage 4.3    â”‚
â”‚  ç»“æ„é‡å»º     â”‚ â”‚  GANå¢å¼º      â”‚ â”‚  æ—¶åºé”å®š     â”‚
â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
â”‚ FFmpeg:      â”‚ â”‚ FFmpeg:      â”‚ â”‚ FFmpeg:      â”‚
â”‚ - lanczos    â”‚ â”‚ - unsharp    â”‚ â”‚ - deflicker  â”‚
â”‚ - unsharp    â”‚ â”‚ - eq         â”‚ â”‚ - éŸ³é¢‘åˆå¹¶   â”‚
â”‚ - hqdn3d     â”‚ â”‚ - hqdn3d     â”‚ â”‚              â”‚
â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
â”‚ è¿›åº¦æ¡æ˜¾ç¤º âœ“ â”‚ â”‚ è¿›åº¦æ¡æ˜¾ç¤º âœ“ â”‚ â”‚ è¿›åº¦æ¡æ˜¾ç¤º âœ“ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®è®¾è®¡å†³ç­–

1. **å…¨æµå¼å¤„ç†**: ä¸æå–å¸§åˆ°ç£ç›˜ï¼Œä½¿ç”¨FFmpegç®¡é“ç›´æ¥å¤„ç†
2. **å®æ—¶è¿›åº¦æ˜¾ç¤º**: æ¯ä¸ªé˜¶æ®µæ˜¾ç¤ºè¿›åº¦æ¡å’Œå¸§è®¡æ•°
3. **çœŸå® GPU è¶…åˆ†**: Stage 4.2 ä¼˜å…ˆä½¿ç”¨ Real-ESRGAN GPUï¼Œå¤±è´¥æ—¶å›é€€åˆ° FFmpeg
4. **ç­–ç•¥é©±åŠ¨**: ä½¿ç”¨ `StrategyDrivenModelManager` è€ŒéåŸºç±» `AIModelManager`

---

## GPU Stage è®¾è®¡ (Stage 4.2 çœŸå® GPU è¶…åˆ†)

### è®¾è®¡è¾¹ç•Œ

| ç±»å‹ | è¯´æ˜ |
|------|------|
| âŒ ä¸è¿½æ±‚ | æµå¼/Zero-copyã€æ˜¾å­˜æé™ä¼˜åŒ–ã€å¤šæ¨¡å‹å¹¶è¡Œ |
| âœ… è¿½æ±‚ | GPU ç¡®å®å‚ä¸åƒç´ è®¡ç®—ã€å¯æ’æ‹”å¯å›é€€ã€å¯éªŒè¯ |

### GPU Stage æ¶æ„
```
ThreeStageVideoEnhancer
â”œâ”€â”€ Stage 4.1 CPU ç»“æ„é‡å»º (FFmpeg lanczos)
â”œâ”€â”€ Stage 4.2 GPU è¶…åˆ†å¢å¼º (Real-ESRGAN) â­ NEW
â”‚   â”œâ”€â”€ ä¼˜å…ˆ: GPUVideoSuperResolver (çœŸå® GPU)
â”‚   â””â”€â”€ å›é€€: FFmpeg æ»¤é•œå¢å¼º
â””â”€â”€ Stage 4.3 CPU æ—¶åº + éŸ³é¢‘åˆæˆ (FFmpeg)
```

### GPU Stage ä»£ç ç»“æ„
```
huaju4k/
â”œâ”€â”€ gpu_stage/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gpu_super_resolver.py   # â­ æ ¸å¿ƒ GPU å¤„ç†å™¨
â”‚   â””â”€â”€ model_manager.py        # æ¨¡å‹ç®¡ç†
â””â”€â”€ requirements-gpu.txt        # GPU ä¾èµ–
```

### GPUVideoSuperResolver æ¥å£
```python
class GPUVideoSuperResolver:
    """
    çœŸå® GPU è§†é¢‘è¶…åˆ†å¤„ç†å™¨
    
    ä½¿ç”¨ Real-ESRGAN è¿›è¡Œ GPU è¶…åˆ†ï¼Œç¡®ä¿ï¼š
    - nvidia-smi æ˜¾ç¤ºæ˜¾å­˜å ç”¨ > 1GB
    - GPU Util æ³¢åŠ¨ 30%~90%
    - è¾“å‡ºè§†é¢‘å¯æ’­æ”¾ä¸”åˆ†è¾¨ç‡æå‡
    """
    
    def __init__(self,
                 model_name: str = "RealESRGAN_x4plus",
                 tile_size: int = 384,  # 6GB æ˜¾å­˜å®‰å…¨å€¼
                 device: str = "cuda"):
        """
        çº¦æŸï¼š
        - åˆå§‹åŒ–å³åŠ è½½æ¨¡å‹åˆ° GPU
        - è¿›ç¨‹ç”Ÿå‘½å‘¨æœŸå†…åªåŠ è½½ä¸€æ¬¡
        """
    
    def enhance_video(self,
                     input_video: str,
                     output_video: str,
                     progress_callback: Optional[Callable] = None) -> bool:
        """
        GPU è¶…åˆ†å¤„ç†æµç¨‹ï¼š
        1. OpenCV è§£ç è§†é¢‘å¸§
        2. frame â†’ torch.Tensor â†’ GPU
        3. Real-ESRGAN forward() æ¨ç†
        4. OpenCV ç¼–ç è¾“å‡ºè§†é¢‘
        
        Returns:
            æˆåŠŸè¿”å› True
        """
```

### GPU æ‰§è¡Œæµç¨‹
```
[input.mp4]
    â”‚
    â–¼
OpenCV decode
    â”‚
raw RGB frame
    â–¼
Torch Tensor (CUDA)   â—€â”€â”€â”€â”€â”€â”€ GPU Util åœ¨è¿™é‡Œä¸Šå‡
    â”‚
    â–¼
Real-ESRGAN forward()
    â”‚
    â–¼
OpenCV encode
    â”‚
    â–¼
[output_sr.mp4]
```

### æ˜¾å­˜ä¸å‚æ•°çº¦æŸ (6GB GPU å®‰å…¨é…ç½®)

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| tile_size | 384 | 6GB æ˜¾å­˜å®‰å…¨å€¼ |
| half | True | FP16 èŠ‚çœæ˜¾å­˜ |
| batch | 1 | å•å¸§å¤„ç† |
| model | RealESRGAN_x4plus | 4x è¶…åˆ† |

### ä¸ä¸»æµç¨‹é›†æˆ
```python
# three_stage_enhancer.py ä¸­çš„é›†æˆé€»è¾‘

def _stage_4_2_controlled_gan_enhancement(self, input_path: str) -> StageResult:
    # 1. å°è¯•çœŸå® GPU è¶…åˆ†
    gpu_success = self._try_real_gpu_super_resolution(input_path, output_path)
    
    if gpu_success:
        return StageResult(success=True, metadata={"gpu_used": True})
    
    # 2. GPU å¤±è´¥ï¼Œå›é€€åˆ° FFmpeg æ»¤é•œ
    return self._fallback_ffmpeg_enhancement(input_path, output_path)

def _try_real_gpu_super_resolution(self, input_path: str, output_path: str) -> bool:
    try:
        from ..gpu_stage import GPUVideoSuperResolver
        
        resolver = GPUVideoSuperResolver(
            model_name="RealESRGAN_x4plus",
            tile_size=384
        )
        
        success = resolver.enhance_video(input_path, output_path)
        resolver.cleanup()
        
        return success
    except Exception:
        logger.warning("GPU SR failed, fallback to CPU")
        return False
```

### GPU æˆåŠŸåˆ¤å®šæ ‡å‡†

è¿è¡Œæ—¶å¿…é¡»åŒæ—¶æ»¡è¶³ï¼š

| æŒ‡æ ‡ | è¦æ±‚ |
|------|------|
| nvidia-smi æ˜¾å­˜ | > 1GB å ç”¨ |
| GPU Util | æ³¢åŠ¨ 30%~90% |
| è¾“å‡ºè§†é¢‘ | å¯æ’­æ”¾ |
| åˆ†è¾¨ç‡ | ç¡®å®æå‡ (å¦‚ 1080p â†’ 4K) |

### å¤±è´¥å›é€€è®¾è®¡
```python
try:
    gpu_stage.enhance_video(input_path, output_path)
except Exception:
    logger.warning("GPU SR failed, fallback to FFmpeg filters")
    # ä½¿ç”¨ FFmpeg æ»¤é•œå¢å¼º
    ffmpeg_enhance(input_path, output_path)
```

**åŸåˆ™**: GPU å¤±è´¥ â‰  æ•´ä¸ªæµç¨‹å¤±è´¥

### æ ¸å¿ƒç»„ä»¶è®¾è®¡

#### 1. StageStructureAnalyzer (Stage 1)
**ä½ç½®**: `huaju4k/analysis/stage_structure_analyzer.py`

**èŒè´£**:
- å¯¹è¾“å…¥è§†é¢‘è¿›è¡Œèˆå°ç»“æ„åˆ†æ
- è¾“å‡ºå®¢è§‚çš„æ•°å€¼åŒ–ç‰¹å¾
- ä¸åšä»»ä½•ä¸»è§‚åˆ¤æ–­æˆ–å†³ç­–

**æ ¸å¿ƒç®—æ³•**:
```python
class StageStructureAnalyzer:
    def analyze_structure(self, video_path: str) -> StructureFeatures:
        """
        åˆ†æèˆå°ç»“æ„ç‰¹å¾
        
        Returns:
            StructureFeatures: åŒ…å«æ‰€æœ‰æ•°å€¼åŒ–ç‰¹å¾çš„æ•°æ®ç±»
        """
        # 1. äº®åº¦ç»“æ„åˆ†æ
        lighting_features = self._analyze_lighting_structure(frames)
        
        # 2. è¾¹ç¼˜å¯†åº¦åˆ†æ  
        edge_features = self._analyze_edge_density(frames)
        
        # 3. å¸§é—´å˜åŒ–åˆ†æ
        motion_features = self._analyze_frame_changes(frames)
        
        # 4. å™ªå£°è¯„ä¼°
        noise_features = self._analyze_noise_level(frames)
        
        return StructureFeatures(
            lighting=lighting_features,
            edge=edge_features,
            motion=motion_features,
            noise=noise_features
        )
```

**è¾“å‡ºæ•°æ®ç»“æ„**:
```python
@dataclass
class StructureFeatures:
    # åŸºç¡€è§†é¢‘ä¿¡æ¯
    resolution: Tuple[int, int]
    fps: float
    duration: float
    total_frames: int
    
    # èˆå°ç»“æ„ç‰¹å¾
    is_static_camera: bool
    highlight_ratio: float      # é«˜å…‰åŒºåŸŸæ¯”ä¾‹
    dark_ratio: float          # æš—éƒ¨åŒºåŸŸæ¯”ä¾‹
    midtone_ratio: float       # ä¸­é—´è°ƒæ¯”ä¾‹
    edge_density: float        # è¾¹ç¼˜å¯†åº¦
    frame_diff_mean: float     # å¸§é—´å˜åŒ–å‡å€¼
    noise_score: float         # å™ªå£°è¯„åˆ†
    
    # åˆ†æå…ƒæ•°æ®
    sample_frames: int         # é‡‡æ ·å¸§æ•°
    analysis_timestamp: datetime
```

#### 2. EnhancementStrategyPlanner (Stage 2)
**ä½ç½®**: `huaju4k/strategy/enhancement_planner.py`

**èŒè´£**:
- å°†Stage 1çš„ç»“æ„ç‰¹å¾ç¿»è¯‘ä¸ºæ‰§è¡Œç­–ç•¥
- ç”Ÿæˆæœºå™¨å¯æ‰§è¡Œçš„ç­–ç•¥JSON
- ä¸ç†è§£è§†é¢‘å†…å®¹ï¼Œåªåšæ•°å€¼æ˜ å°„

**ç­–ç•¥ç”Ÿæˆé€»è¾‘**:
```python
class EnhancementStrategyPlanner:
    def generate_strategy(self, features: StructureFeatures) -> EnhancementStrategy:
        """
        åŸºäºç»“æ„ç‰¹å¾ç”Ÿæˆå¢å¼ºç­–ç•¥
        
        Args:
            features: Stage 1è¾“å‡ºçš„ç»“æ„ç‰¹å¾
            
        Returns:
            EnhancementStrategy: å®Œæ•´çš„æ‰§è¡Œç­–ç•¥
        """
        # 1. åˆ†è¾¨ç‡è·¯å¾„è§„åˆ’
        resolution_plan = self._plan_resolution_path(features.resolution)
        
        # 2. GANå…è®¸åº¦è®¡ç®—
        gan_policy = self._calculate_gan_policy(features)
        
        # 3. åˆ†å±‚å¤„ç†ç­–ç•¥
        layer_strategy = self._generate_layer_strategy(features)
        
        # 4. æ—¶åºé”å®šç­–ç•¥
        temporal_strategy = self._generate_temporal_strategy(features)
        
        # 5. å†…å­˜ç®¡ç†ç­–ç•¥
        memory_policy = self._generate_memory_policy(features)
        
        return EnhancementStrategy(
            resolution_plan=resolution_plan,
            gan_policy=gan_policy,
            layer_strategy=layer_strategy,
            temporal_strategy=temporal_strategy,
            memory_policy=memory_policy
        )
```

**ç­–ç•¥æ•°æ®ç»“æ„**:
```python
@dataclass
class EnhancementStrategy:
    # åˆ†è¾¨ç‡å¤„ç†è·¯å¾„
    resolution_plan: List[str]  # ["x2", "x2"] æˆ– ["x2"]
    
    # åˆ†å±‚å¤„ç†ç­–ç•¥
    layer_strategy: Dict[str, LayerConfig]
    
    # GANæ§åˆ¶ç­–ç•¥
    gan_policy: GANPolicy
    
    # æ—¶åºå¤„ç†ç­–ç•¥
    temporal_strategy: TemporalConfig
    
    # å†…å­˜ç®¡ç†ç­–ç•¥
    memory_policy: MemoryConfig
    
    # éŸ³é¢‘å¤„ç†ç­–ç•¥
    audio_strategy: AudioConfig
    
    # ç­–ç•¥å…ƒæ•°æ®
    strategy_version: str
    generation_timestamp: datetime
    source_features_hash: str

@dataclass
class LayerConfig:
    structure_sr: bool          # æ˜¯å¦å¯ç”¨ç»“æ„é‡å»º
    gan_strength: str          # "off", "weak", "medium", "strong"
    post_smooth: bool          # æ˜¯å¦åå¤„ç†å¹³æ»‘
    detail_limit: bool         # æ˜¯å¦é™åˆ¶ç»†èŠ‚ç”Ÿæˆ

@dataclass
class GANPolicy:
    global_allowed: bool       # å…¨å±€GANå¼€å…³
    strength: str             # "weak", "medium", "strong"
    highlight_threshold: float # é«˜å…‰é˜ˆå€¼ (0.85)
    shadow_threshold: float   # æš—éƒ¨é˜ˆå€¼ (0.15)
    edge_threshold: float     # è¾¹ç¼˜å¯†åº¦é˜ˆå€¼ (0.1)
    motion_threshold: float   # è¿åŠ¨æ£€æµ‹é˜ˆå€¼ (0.05)
    highlight_forbidden: bool  # é«˜å…‰åŒºåŸŸç¦ç”¨
    dark_limit: bool          # æš—éƒ¨é™åˆ¶

@dataclass
class TemporalConfig:
    background_lock: bool      # èƒŒæ™¯é”å®š
    strength: str             # "low", "medium", "high"
    motion_threshold: float   # è¿åŠ¨æ£€æµ‹é˜ˆå€¼
    optical_flow_enabled: bool # æ˜¯å¦å¯ç”¨å…‰æµ
    smoothing_alpha: float    # å¸§é—´å¹³æ»‘ç³»æ•° (0.3)

@dataclass
class MemoryConfig:
    max_model_loaded: int     # æœ€å¤§åŒæ—¶åŠ è½½æ¨¡å‹æ•°
    tile_size: int           # ç“¦ç‰‡å¤§å°
    batch_size: int          # æ‰¹å¤„ç†å¤§å°
    use_fp16: bool           # æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦
    max_workers: int         # CPUå¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°

@dataclass
class AudioConfig:
    source_separation_enabled: bool  # æ˜¯å¦å¯ç”¨éŸ³æºåˆ†ç¦»
    dialogue_enhancement: float     # å¯¹ç™½å¢å¼ºå¼ºåº¦ (0.0-1.0)
    music_processing: str          # éŸ³ä¹å¤„ç†æ¨¡å¼ ("preserve", "enhance")
    ambient_processing: str        # ç¯å¢ƒéŸ³å¤„ç†æ¨¡å¼ ("preserve", "spatial")
    master_settings: Dict[str, Any] # æ¯ç‰ˆçº§é‡æ··è®¾ç½®
```

#### 3. StrategyDrivenModelManager (Stage 3å‡çº§)
**ä½ç½®**: `huaju4k/core/ai_model_manager.py` (å‡çº§ç°æœ‰)

**æ ¸å¿ƒæ”¹è¿›**:
- ç­–ç•¥é©±åŠ¨çš„æ¨¡å‹è°ƒåº¦
- ä¸¥æ ¼çš„æ˜¾å­˜ç®¡ç† (6GBçº¦æŸ)
- å³ç”¨å³è½½ï¼Œç”¨å®Œå³å¸
- å•æ¨¡å‹çº¦æŸ (cache_size=1)

```python
class StrategyDrivenModelManager(AIModelManager):
    """
    ç­–ç•¥é©±åŠ¨çš„AIæ¨¡å‹ç®¡ç†å™¨
    
    ç»§æ‰¿è‡ªAIModelManagerï¼Œæ·»åŠ :
    - set_strategy(): è®¾ç½®å¢å¼ºç­–ç•¥
    - execute_strategy_phase(): æ‰§è¡Œç­–ç•¥é˜¶æ®µ
    - predict_masked(): åŒºåŸŸå¢å¼ºé¢„æµ‹
    - GPUMemoryMonitor: GPUå†…å­˜ç›‘æ§
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.current_strategy: Optional[EnhancementStrategy] = None
        self.gpu_memory_monitor = GPUMemoryMonitor()
        self.cache_size = 1  # å¼ºåˆ¶å•æ¨¡å‹çº¦æŸ
    
    def set_strategy(self, strategy: EnhancementStrategy) -> None:
        """
        è®¾ç½®å½“å‰å¢å¼ºç­–ç•¥
        
        Args:
            strategy: å¢å¼ºç­–ç•¥é…ç½®
            
        æ³¨æ„: VideoEnhancementProcessor å¿…é¡»ä½¿ç”¨æ­¤ç±»è€ŒéåŸºç±» AIModelManager
        """
        self.current_strategy = strategy
        logger.info(f"Strategy set: GAN={strategy.gan_policy.global_allowed}, "
                   f"temporal_lock={strategy.temporal_strategy.background_lock}")
    
    def execute_strategy_phase(self, phase: str) -> bool:
        """
        æ‰§è¡Œç­–ç•¥ä¸­çš„ç‰¹å®šé˜¶æ®µ
        
        Args:
            phase: "structure_sr", "gan_enhance", "temporal_lock"
            
        Returns:
            bool: æ‰§è¡ŒæˆåŠŸæ ‡å¿—
        """
        if not self.current_strategy:
            logger.warning("No strategy set, using default model selection")
            return self.load_model('opencv_cubic')
        
        # ç¡®å®šå½“å‰é˜¶æ®µéœ€è¦çš„æ¨¡å‹
        required_model = self._get_required_model_for_phase(phase)
        
        # åˆ‡æ¢æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if required_model != self.current_model_name:
            return self._switch_model(required_model)
        
        return True
    
    def _switch_model(self, model_name: str) -> bool:
        """
        å®‰å…¨åˆ‡æ¢æ¨¡å‹ï¼Œç¡®ä¿æ˜¾å­˜é‡Šæ”¾
        """
        # å¸è½½å½“å‰æ¨¡å‹
        if self.current_model:
            self.current_model.unload()
            self.current_model = None
            self.current_model_name = None
            
            # å¼ºåˆ¶GPUå†…å­˜æ¸…ç†
            self.gpu_memory_monitor.force_gpu_memory_cleanup()
        
        # åŠ è½½æ–°æ¨¡å‹
        return self.load_model(model_name)


class GPUMemoryMonitor:
    """
    GPUå†…å­˜ç›‘æ§å™¨ - 6GBæ˜¾å­˜çº¦æŸ
    """
    
    def __init__(self, max_gpu_memory_mb: int = 5500):
        self.max_gpu_memory_mb = max_gpu_memory_mb  # é¢„ç•™500MBç¼“å†²
    
    def check_gpu_memory_available(self) -> int:
        """æ£€æŸ¥å¯ç”¨GPUå†…å­˜ (MB)"""
        try:
            import torch
            if torch.cuda.is_available():
                total = torch.cuda.get_device_properties(0).total_memory
                allocated = torch.cuda.memory_allocated(0)
                cached = torch.cuda.memory_reserved(0)
                
                used_mb = (allocated + cached) // (1024 * 1024)
                effective_total = min(total // (1024 * 1024), self.max_gpu_memory_mb)
                
                return max(0, effective_total - used_mb)
            return 0
        except:
            return 0
    
    def force_gpu_memory_cleanup(self) -> None:
        """å¼ºåˆ¶GPUå†…å­˜æ¸…ç†"""
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        except:
            pass
```

**é‡è¦**: `VideoEnhancementProcessor` å¿…é¡»ä½¿ç”¨ `StrategyDrivenModelManager` è€ŒéåŸºç±» `AIModelManager`ï¼š
```python
# æ­£ç¡® âœ“
self.ai_model_manager = StrategyDrivenModelManager(
    models_dir=self.config.get('models_dir', './models'),
    cache_size=self.config.get('model_cache_size', 2)
)

# é”™è¯¯ âœ— - ä¼šå¯¼è‡´ 'AIModelManager' object has no attribute 'set_strategy'
# self.ai_model_manager = AIModelManager(...)
```

#### 4. ThreeStageVideoEnhancer (Stage 4é‡æ„)
**ä½ç½®**: `huaju4k/core/three_stage_enhancer.py`

**æ ¸å¿ƒè®¾è®¡åŸåˆ™**:
- **å…¨æµå¼å¤„ç†**: ä½¿ç”¨ FFmpeg ç®¡é“ï¼Œä¸æå–å¸§åˆ°ç£ç›˜
- **å®æ—¶è¿›åº¦æ˜¾ç¤º**: æ¯ä¸ªé˜¶æ®µæ˜¾ç¤ºè¿›åº¦æ¡å’Œå¸§è®¡æ•°
- **å›é€€æœºåˆ¶**: Real-ESRGAN ä¸å¯ç”¨æ—¶ä½¿ç”¨ FFmpeg æ»¤é•œå¢å¼º

**ä¸‰é˜¶æ®µå¤„ç†æµç¨‹**:

```python
class ThreeStageVideoEnhancer:
    def enhance_video(self, input_path: str, output_path: str,
                     strategy: EnhancementStrategy,
                     progress_callback: Optional[Callable] = None) -> bool:
        """
        ä¸‰é˜¶æ®µè§†é¢‘å¢å¼ºå¤„ç† - å…¨æµå¼FFmpegå®ç°
        
        Stage 4.1: ç»“æ„é‡å»º (FFmpeg lanczos + unsharp + hqdn3d)
        Stage 4.2: å—æ§GANå¢å¼º (FFmpegæ»¤é•œæˆ–Real-ESRGAN)
        Stage 4.3: æ—¶åºé”å®š + éŸ³é¢‘åˆå¹¶ (FFmpeg)
        
        Returns:
            bool: å¤„ç†æˆåŠŸæ ‡å¿—
        """
        # Stage 4.1: ç»“æ„é‡å»º
        structure_result = self._stage_4_1_structure_reconstruction(input_path)
        if not structure_result.success:
            return False
        
        # Stage 4.2: å—æ§GANå¢å¼º
        gan_result = self._stage_4_2_controlled_gan_enhancement(
            structure_result.output_path
        )
        if not gan_result.success:
            return False
        
        # Stage 4.3: æ—¶åºé”å®š + éŸ³é¢‘åˆå¹¶
        temporal_result = self._stage_4_3_temporal_locking(
            gan_result.output_path, output_path
        )
        
        return temporal_result.success
```

**Stage 4.1: ç»“æ„é‡å»º (FFmpegæµå¼å¤„ç†)**:
```python
def _stage_4_1_structure_reconstruction(self, input_path: str) -> StageResult:
    """
    Stage 4.1: ä½¿ç”¨FFmpegè¿›è¡Œç»“æ„é‡å»º
    
    å¤„ç†æµç¨‹:
    1. æ ¹æ®resolution_planæ‰§è¡Œå¤šæ­¥æ”¾å¤§ (å¦‚ 1080p -> 2K -> 4K)
    2. æ¯æ­¥ä½¿ç”¨ lanczos + unsharp + hqdn3d æ»¤é•œ
    3. å®æ—¶æ˜¾ç¤ºè¿›åº¦æ¡
    """
    resolution_plan = self.current_strategy.resolution_plan  # ["x2", "x2"]
    
    for step_idx, step in enumerate(resolution_plan):
        # FFmpegæ»¤é•œé“¾: é«˜è´¨é‡æ”¾å¤§ + é”åŒ– + é™å™ª
        filter_complex = (
            f"scale={target_width}:{target_height}:flags=lanczos,"
            f"unsharp=5:5:0.8:5:5:0.4,"
            f"hqdn3d=1.5:1.5:6:6"
        )
        
        cmd = [
            'ffmpeg', '-y', '-i', current_input,
            '-vf', filter_complex,
            '-c:v', 'libx264', '-preset', 'medium', '-crf', '18',
            '-progress', 'pipe:1',  # è¿›åº¦è¾“å‡º
            step_output
        ]
        
        # å®æ—¶è¿›åº¦æ˜¾ç¤º
        print(f"\nğŸ¬ Stage 4.1 - ç»“æ„é‡å»º æ­¥éª¤ {step_idx+1}/{len(resolution_plan)}")
        print(f"   åˆ†è¾¨ç‡: {current_width}x{current_height} -> {target_width}x{target_height}")
        
        # è§£æFFmpegè¿›åº¦è¾“å‡ºå¹¶æ˜¾ç¤ºè¿›åº¦æ¡
        for line in process.stdout:
            if line.startswith('frame='):
                frames_processed = int(line.strip().split('=')[1])
                progress = frames_processed / total_frames
                bar = 'â–ˆ' * int(40 * progress) + 'â–‘' * (40 - int(40 * progress))
                print(f"\r   è¿›åº¦: [{bar}] {progress*100:.1f}%", end='', flush=True)
```

**Stage 4.2: GANå¢å¼º (FFmpegæ»¤é•œå›é€€)**:
```python
def _stage_4_2_controlled_gan_enhancement(self, input_path: str) -> StageResult:
    """
    Stage 4.2: å—æ§GANå¢å¼º
    
    å®é™…å®ç°:
    - å¦‚æœReal-ESRGANå¯ç”¨: ä½¿ç”¨AIæ¨¡å‹å¢å¼º
    - å¦‚æœä¸å¯ç”¨: ä½¿ç”¨FFmpegé«˜çº§æ»¤é•œæ¨¡æ‹ŸGANæ•ˆæœ
    
    FFmpegæ»¤é•œæ ¹æ®gan_strengthè°ƒæ•´:
    - weak:   unsharp=3:3:0.5, eq=contrast=1.01
    - medium: unsharp=5:5:0.8, eq=contrast=1.03:saturation=1.05
    - strong: unsharp=7:7:1.2, eq=contrast=1.05:saturation=1.1
    """
    if not self.current_strategy.gan_policy.global_allowed:
        return StageResult(output_path=input_path, success=True, skipped=True)
    
    gan_strength = self.current_strategy.gan_policy.strength
    
    # æ ¹æ®å¼ºåº¦é€‰æ‹©æ»¤é•œå‚æ•°
    if gan_strength == "strong":
        filter_complex = "unsharp=7:7:1.2:7:7:0.6,eq=contrast=1.05:brightness=0.02:saturation=1.1"
    elif gan_strength == "medium":
        filter_complex = "unsharp=5:5:0.8:5:5:0.4,eq=contrast=1.03:brightness=0.01:saturation=1.05"
    else:  # weak
        filter_complex = "unsharp=3:3:0.5:3:3:0.3,eq=contrast=1.01:saturation=1.02"
    
    print(f"\nğŸ¨ Stage 4.2 - GANå¢å¼º (å¼ºåº¦: {gan_strength})")
    # ... FFmpegå¤„ç† + è¿›åº¦æ˜¾ç¤º
```

**Stage 4.3: æ—¶åºé”å®š + éŸ³é¢‘åˆå¹¶**:
```python
def _stage_4_3_temporal_locking(self, input_path: str, 
                               final_output_path: str) -> StageResult:
    """
    Stage 4.3: æ—¶åºé”å®š + éŸ³é¢‘åˆå¹¶
    
    å¤„ç†æµç¨‹:
    1. æ ¹æ®temporal_strategyé€‰æ‹©æ»¤é•œ
    2. ä»åŸå§‹è§†é¢‘æå–éŸ³é¢‘å¹¶åˆå¹¶
    3. è¾“å‡ºæœ€ç»ˆ4Kè§†é¢‘
    
    æ—¶åºæ»¤é•œé€‰æ‹©:
    - high strength + background_lock: mpdecimate
    - medium strength: deflicker
    - low strength: null (ç›´é€š)
    """
    temporal_config = self.current_strategy.temporal_strategy
    
    # é€‰æ‹©æ—¶åºæ»¤é•œ
    if temporal_config.background_lock and temporal_config.strength == "high":
        temporal_filter = "mpdecimate,setpts=N/FRAME_RATE/TB"
    elif temporal_config.strength == "medium":
        temporal_filter = "deflicker=mode=pm:size=5"
    else:
        temporal_filter = "null"
    
    # åˆå¹¶è§†é¢‘å’ŒåŸå§‹éŸ³é¢‘
    cmd = [
        'ffmpeg', '-y',
        '-i', input_path,           # å¢å¼ºåçš„è§†é¢‘
        '-i', original_video,       # åŸå§‹è§†é¢‘(éŸ³é¢‘æº)
        '-filter_complex', f"[0:v]{temporal_filter}[v]",
        '-map', '[v]', '-map', '1:a?',
        '-c:v', 'libx264', '-c:a', 'aac',
        '-progress', 'pipe:1',
        final_output_path
    ]
    
    print(f"\nğŸ”’ Stage 4.3 - æ—¶åºé”å®š + éŸ³é¢‘åˆå¹¶")
    print(f"   éŸ³é¢‘: {'æœ‰' if has_audio else 'æ— '}")
    # ... FFmpegå¤„ç† + è¿›åº¦æ˜¾ç¤º
```
    
    def _generate_multi_dimensional_safe_mask(self, frame: np.ndarray, 
                                            previous_frame: Optional[np.ndarray],
                                            gan_policy: GANPolicy) -> np.ndarray:
        """
        ç”Ÿæˆå¤šç»´åº¦GANå®‰å…¨åŒºåŸŸmask
        
        ç»¼åˆè€ƒè™‘ï¼š
        1. äº®åº¦æ’é™¤ï¼ˆé«˜å…‰å’Œæš—éƒ¨åŒºåŸŸç¦ç”¨GANï¼‰
        2. è¾¹ç¼˜å¯†åº¦é™åˆ¶ï¼ˆåªåœ¨è¾¹ç¼˜å¯†åº¦é«˜çš„åŒºåŸŸå…è®¸ç»†èŠ‚å¢å¼ºï¼‰
        3. è¿åŠ¨æ£€æµ‹é™åˆ¶ï¼ˆå¿«é€Ÿç§»åŠ¨åŒºåŸŸå…è®¸å°å¹…å¢å¼ºï¼‰
        """
        height, width = frame.shape[:2]
        
        # 1. äº®åº¦æ’é™¤mask
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        frame_normalized = frame_gray.astype(np.float32) / 255.0
        
        # é«˜å…‰å’Œæš—éƒ¨é˜ˆå€¼
        highlight_mask = frame_normalized > gan_policy.highlight_threshold
        shadow_mask = frame_normalized < gan_policy.shadow_threshold
        
        # 2. è¾¹ç¼˜å¯†åº¦mask
        edges = cv2.Canny(frame_gray, 50, 150)
        
        # è®¡ç®—å±€éƒ¨è¾¹ç¼˜å¯†åº¦
        kernel = np.ones((15, 15), np.uint8)  # 15x15é‚»åŸŸ
        edge_density = cv2.filter2D(edges.astype(np.float32), -1, kernel) / (15 * 15 * 255)
        edge_mask = edge_density > gan_policy.edge_threshold
        
        # 3. è¿åŠ¨æ£€æµ‹mask
        motion_mask = np.ones((height, width), dtype=bool)  # é»˜è®¤å…è®¸
        if previous_frame is not None:
            # å¸§é—´å·®åˆ†æ£€æµ‹è¿åŠ¨
            prev_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)
            frame_diff = cv2.absdiff(frame_gray, prev_gray).astype(np.float32) / 255.0
            motion_mask = frame_diff > gan_policy.motion_threshold
        
        # 4. ç»¼åˆå®‰å…¨åŒºåŸŸmask
        # å…è®¸GANçš„åŒºåŸŸï¼šæœ‰è¾¹ç¼˜ AND éé«˜å…‰ AND éæš—éƒ¨ AND (æœ‰è¿åŠ¨æˆ–é™æ€èƒŒæ™¯)
        safe_mask = edge_mask & (~highlight_mask) & (~shadow_mask)
        
        # æ ¹æ®GANå¼ºåº¦è°ƒæ•´mask
        if gan_policy.strength == 'weak':
            # å¼±å¼ºåº¦ï¼šåªåœ¨æ˜ç¡®çš„è¾¹ç¼˜åŒºåŸŸ
            safe_mask = safe_mask & motion_mask
        elif gan_policy.strength == 'medium':
            # ä¸­ç­‰å¼ºåº¦ï¼šè¾¹ç¼˜åŒºåŸŸ + éƒ¨åˆ†é™æ€åŒºåŸŸ
            safe_mask = safe_mask | (motion_mask & (~highlight_mask))
        elif gan_policy.strength == 'strong':
            # å¼ºå¼ºåº¦ï¼šæ›´å¤§èŒƒå›´ï¼Œä½†ä»é¿å…é«˜å…‰
            safe_mask = safe_mask | (~highlight_mask & ~shadow_mask)
        
        # å½¢æ€å­¦æ“ä½œå¹³æ»‘mask
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        safe_mask = cv2.morphologyEx(safe_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)
        safe_mask = cv2.morphologyEx(safe_mask, cv2.MORPH_OPEN, kernel)
        
        return safe_mask.astype(bool)
    
    def _blend_enhanced_regions(self, original_frame: np.ndarray, 
                              enhanced_regions: List[Dict], 
                              safe_mask: np.ndarray) -> np.ndarray:
        """
        å°†å¢å¼ºåŒºåŸŸæ··åˆå›åŸå§‹å¸§
        """
        result_frame = original_frame.copy()
        
        for region_data in enhanced_regions:
            enhanced_region = region_data['region']
            region_mask = region_data['mask']
            x, y, w, h = region_data['bbox']
            
            # å°†å¢å¼ºåŒºåŸŸæ··åˆå›åŸå§‹å¸§
            for c in range(3):  # RGBä¸‰é€šé“
                result_frame[y:y+h, x:x+w, c] = np.where(
                    region_mask,
                    enhanced_region[:, :, c],
                    result_frame[y:y+h, x:x+w, c]
                )
        
        return result_frame
```

#### 5. TemporalLockProcessor (æ—¶åºé”å®šå¤„ç†å™¨)
**ä½ç½®**: `huaju4k/core/temporal_lock_processor.py`

**è®¾è®¡è¯´æ˜**:
å½“å‰å®ç°ä½¿ç”¨ FFmpeg å†…ç½®æ»¤é•œè¿›è¡Œæ—¶åºå¤„ç†ï¼Œè€Œé OpenCV å…‰æµã€‚
è¿™æ˜¯å› ä¸º FFmpeg æ»¤é•œåœ¨æµå¼å¤„ç†ä¸­æ›´é«˜æ•ˆï¼Œä¸”ä¸éœ€è¦å°†å¸§æå–åˆ°å†…å­˜ã€‚

**FFmpeg æ—¶åºæ»¤é•œé€‰æ‹©**:
```python
class TemporalLockProcessor:
    def get_temporal_filter(self, temporal_config: TemporalConfig) -> str:
        """
        æ ¹æ®é…ç½®è¿”å›FFmpegæ—¶åºæ»¤é•œ
        
        æ»¤é•œé€‰æ‹©é€»è¾‘:
        - background_lock=True + strength=high: mpdecimate (å»é™¤é‡å¤å¸§)
        - strength=medium: deflicker (å»é—ªçƒ)
        - strength=low: null (ç›´é€šï¼Œä¸å¤„ç†)
        """
        if temporal_config.background_lock and temporal_config.strength == "high":
            return "mpdecimate,setpts=N/FRAME_RATE/TB"
        elif temporal_config.strength == "medium":
            return "deflicker=mode=pm:size=5"
        else:
            return "null"
```

**å¤‡ç”¨ OpenCV å…‰æµå®ç°** (ç”¨äºéœ€è¦ç²¾ç»†æ§åˆ¶çš„åœºæ™¯):
```python
def _apply_optical_flow_stabilization(self, current_frame: np.ndarray,
                                    previous_frame: np.ndarray) -> np.ndarray:
    """
    ä½¿ç”¨å…‰æµè¿›è¡ŒèƒŒæ™¯ç¨³å®š (å¤‡ç”¨æ–¹æ¡ˆ)
    
    æ³¨æ„: å½“å‰ä¸»æµç¨‹ä½¿ç”¨FFmpegæ»¤é•œï¼Œæ­¤æ–¹æ³•ä½œä¸ºå¤‡ç”¨
    """
    # è®¡ç®—å…‰æµ
    flow = cv2.calcOpticalFlowFarneback(
        previous_gray, current_gray, None, 
        0.5, 3, 15, 3, 5, 1.2, 0
    )
    
    # æ£€æµ‹è¿åŠ¨åŒºåŸŸ
    motion_magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)
    motion_mask = motion_magnitude > self.config.motion_threshold
    
    # åˆå¹¶è¿åŠ¨åŒºåŸŸå’Œç¨³å®šèƒŒæ™¯
    return stabilized_frame
```

#### 6. MasterGradeAudioEnhancer (Stage 5å‡çº§)
**ä½ç½®**: `huaju4k/audio/master_grade_enhancer.py`

**å…·ä½“åº“è°ƒç”¨çš„æ¯ç‰ˆçº§éŸ³é¢‘å¤„ç†**:
```python
import subprocess
import os
from pathlib import Path
from typing import Dict, Any, Optional
from pydub import AudioSegment
import librosa
import noisereduce as nr
import numpy as np

class MasterGradeAudioEnhancer:
    def __init__(self):
        self.temp_dir = Path("/tmp/huaju4k_audio")
        self.temp_dir.mkdir(exist_ok=True)
    
    def enhance_audio(self, video_path: str, 
                     strategy: EnhancementStrategy) -> AudioResult:
        """
        æ¯ç‰ˆçº§éŸ³é¢‘å¢å¼ºä¸»æµç¨‹ - å…·ä½“å®ç°ç‰ˆæœ¬
        """
        if not strategy.audio_strategy.source_separation_enabled:
            # ç®€å•éŸ³é¢‘å¢å¼º
            return self._simple_audio_enhancement(video_path, strategy)
        
        # 1. éŸ³è½¨æå–
        audio_path = self._extract_audio_with_ffmpeg(video_path)
        
        # 2. éŸ³æºåˆ†ç¦»
        separated_tracks = self._separate_audio_sources(audio_path)
        
        # 3. åˆ†è½¨å¤„ç†
        enhanced_dialogue = self._enhance_dialogue(
            separated_tracks['vocals'], 
            strategy.audio_strategy.dialogue_enhancement
        )
        
        enhanced_music = self._process_music(
            separated_tracks['accompaniment'],
            strategy.audio_strategy.music_processing
        )
        
        enhanced_ambient = self._process_ambient(
            separated_tracks.get('ambient', separated_tracks['accompaniment']),
            strategy.audio_strategy.ambient_processing
        )
        
        # 4. æ¯ç‰ˆçº§é‡æ··
        master_audio_path = self._master_grade_remix(
            enhanced_dialogue, enhanced_music, enhanced_ambient,
            strategy.audio_strategy.master_settings
        )
        
        return AudioResult(
            output_path=master_audio_path,
            quality_metrics=self._calculate_audio_quality_metrics(master_audio_path)
        )
    
    def _extract_audio_with_ffmpeg(self, video_path: str) -> str:
        """
        ä½¿ç”¨FFmpegæå–éŸ³è½¨
        """
        audio_path = self.temp_dir / "extracted_audio.wav"
        
        cmd = [
            'ffmpeg', '-i', video_path,
            '-vn',  # ä¸è¦è§†é¢‘
            '-acodec', 'pcm_s16le',  # 16ä½PCM
            '-ar', '44100',  # 44.1kHzé‡‡æ ·ç‡
            '-ac', '2',  # ç«‹ä½“å£°
            '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
            str(audio_path)
        ]
        
        subprocess.run(cmd, check=True, capture_output=True)
        return str(audio_path)
    
    def _separate_audio_sources(self, audio_path: str) -> Dict[str, str]:
        """
        ä½¿ç”¨Spleeterè¿›è¡ŒéŸ³æºåˆ†ç¦»
        """
        try:
            from spleeter.separator import Separator
            
            # åˆå§‹åŒ–åˆ†ç¦»å™¨ (äººå£° + ä¼´å¥)
            separator = Separator('spleeter:2stems-16kHz')
            
            # åˆ†ç¦»éŸ³é¢‘
            output_dir = self.temp_dir / "separated"
            output_dir.mkdir(exist_ok=True)
            
            separator.separate_to_file(audio_path, str(output_dir))
            
            # è¿”å›åˆ†ç¦»åçš„æ–‡ä»¶è·¯å¾„
            audio_name = Path(audio_path).stem
            return {
                'vocals': str(output_dir / audio_name / "vocals.wav"),
                'accompaniment': str(output_dir / audio_name / "accompaniment.wav")
            }
            
        except ImportError:
            # å›é€€åˆ°ç®€å•çš„é¢‘åŸŸåˆ†ç¦»
            return self._simple_vocal_separation(audio_path)
    
    def _simple_vocal_separation(self, audio_path: str) -> Dict[str, str]:
        """
        ç®€å•çš„äººå£°åˆ†ç¦»ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
        """
        # åŠ è½½éŸ³é¢‘
        y, sr = librosa.load(audio_path, sr=44100, mono=False)
        
        if y.ndim == 1:
            # å•å£°é“ï¼Œæ— æ³•åˆ†ç¦»
            vocals_path = self.temp_dir / "vocals_mono.wav"
            accompaniment_path = self.temp_dir / "accompaniment_mono.wav"
            
            # å¤åˆ¶åŸéŸ³é¢‘
            librosa.output.write_wav(str(vocals_path), y, sr)
            librosa.output.write_wav(str(accompaniment_path), y * 0.3, sr)  # é™ä½ä¼´å¥éŸ³é‡
        else:
            # ç«‹ä½“å£°ï¼Œä½¿ç”¨ä¸­å¤®å£°é“åˆ†ç¦»
            vocals = y[0] - y[1]  # å·¦å³å£°é“å·®å€¼ï¼ˆç²—ç•¥çš„äººå£°æå–ï¼‰
            accompaniment = (y[0] + y[1]) / 2  # å·¦å³å£°é“å¹³å‡ï¼ˆä¼´å¥ï¼‰
            
            vocals_path = self.temp_dir / "vocals_separated.wav"
            accompaniment_path = self.temp_dir / "accompaniment_separated.wav"
            
            librosa.output.write_wav(str(vocals_path), vocals, sr)
            librosa.output.write_wav(str(accompaniment_path), accompaniment, sr)
        
        return {
            'vocals': str(vocals_path),
            'accompaniment': str(accompaniment_path)
        }
    
    def _enhance_dialogue(self, vocals_path: str, enhancement_strength: float) -> str:
        """
        å¯¹ç™½å¢å¼ºå¤„ç†
        """
        # åŠ è½½äººå£°éŸ³é¢‘
        audio = AudioSegment.from_wav(vocals_path)
        
        # 1. é™å™ªå¤„ç†
        y, sr = librosa.load(vocals_path, sr=44100)
        
        # ä½¿ç”¨noisereduceè¿›è¡Œé™å™ª
        reduced_noise = nr.reduce_noise(y=y, sr=sr, prop_decrease=enhancement_strength * 0.8)
        
        # 2. é¢„åŠ é‡ï¼ˆå¢å¼ºé«˜é¢‘ï¼‰
        if enhancement_strength > 0.5:
            reduced_noise = librosa.effects.preemphasis(reduced_noise, coef=0.97)
        
        # 3. åŠ¨æ€èŒƒå›´å‹ç¼©
        # ç®€å•çš„è½¯é™åˆ¶å™¨
        threshold = 0.8
        ratio = 4.0
        compressed = np.where(
            np.abs(reduced_noise) > threshold,
            np.sign(reduced_noise) * (threshold + (np.abs(reduced_noise) - threshold) / ratio),
            reduced_noise
        )
        
        # ä¿å­˜å¢å¼ºåçš„å¯¹ç™½
        enhanced_path = self.temp_dir / "enhanced_dialogue.wav"
        librosa.output.write_wav(str(enhanced_path), compressed, sr)
        
        return str(enhanced_path)
    
    def _process_music(self, music_path: str, processing_mode: str) -> str:
        """
        éŸ³ä¹å¤„ç†
        """
        if processing_mode == "preserve":
            # ä¿æŒåŸæ ·
            return music_path
        elif processing_mode == "enhance":
            # è½»å¾®å¢å¼º
            audio = AudioSegment.from_wav(music_path)
            
            # è½»å¾®çš„EQè°ƒæ•´
            enhanced = audio.low_pass_filter(8000).high_pass_filter(80)
            
            enhanced_path = self.temp_dir / "enhanced_music.wav"
            enhanced.export(str(enhanced_path), format="wav")
            return str(enhanced_path)
        
        return music_path
    
    def _process_ambient(self, ambient_path: str, processing_mode: str) -> str:
        """
        ç¯å¢ƒéŸ³å¤„ç†
        """
        if processing_mode == "preserve":
            return ambient_path
        elif processing_mode == "spatial":
            # å¢å¼ºç©ºé—´æ„Ÿ
            audio = AudioSegment.from_wav(ambient_path)
            
            # è½»å¾®çš„æ··å“æ•ˆæœï¼ˆç®€åŒ–ç‰ˆï¼‰
            # å®é™…å®ç°ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç©ºé—´éŸ³é¢‘å¤„ç†
            enhanced = audio + 2  # è½»å¾®å¢ç›Š
            
            enhanced_path = self.temp_dir / "enhanced_ambient.wav"
            enhanced.export(str(enhanced_path), format="wav")
            return str(enhanced_path)
        
        return ambient_path
    
    def _master_grade_remix(self, dialogue_path: str, music_path: str, 
                          ambient_path: str, master_settings: Dict[str, Any]) -> str:
        """
        æ¯ç‰ˆçº§é‡æ··
        """
        # åŠ è½½æ‰€æœ‰éŸ³è½¨
        dialogue = AudioSegment.from_wav(dialogue_path)
        music = AudioSegment.from_wav(music_path)
        ambient = AudioSegment.from_wav(ambient_path)
        
        # éŸ³é‡å¹³è¡¡
        dialogue_gain = master_settings.get('dialogue_gain', 0)  # dB
        music_gain = master_settings.get('music_gain', -6)       # dB
        ambient_gain = master_settings.get('ambient_gain', -12)  # dB
        
        dialogue = dialogue + dialogue_gain
        music = music + music_gain
        ambient = ambient + ambient_gain
        
        # æ··åˆéŸ³è½¨
        # ç¡®ä¿æ‰€æœ‰éŸ³è½¨é•¿åº¦ä¸€è‡´
        max_length = max(len(dialogue), len(music), len(ambient))
        
        dialogue = dialogue[:max_length]
        music = music[:max_length] 
        ambient = ambient[:max_length]
        
        # å åŠ æ··åˆ
        master_audio = dialogue.overlay(music).overlay(ambient)
        
        # æ¯ç‰ˆçº§å¤„ç†
        # 1. é™åˆ¶å™¨é˜²æ­¢å‰Šæ³¢
        master_audio = master_audio.normalize(headroom=1.0)
        
        # 2. æœ€ç»ˆéŸ³é‡è°ƒæ•´
        target_lufs = master_settings.get('target_lufs', -23)  # EBU R128æ ‡å‡†
        # ç®€åŒ–çš„å“åº¦è°ƒæ•´ï¼ˆå®é™…åº”ä½¿ç”¨ä¸“ä¸šå“åº¦æµ‹é‡ï¼‰
        master_audio = master_audio.normalize(headroom=abs(target_lufs))
        
        # å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘
        master_path = self.temp_dir / "master_audio.wav"
        master_audio.export(str(master_path), format="wav")
        
        return str(master_path)
```

### æ€§èƒ½ä¼˜åŒ–è®¾è®¡

#### å®æ—¶è¿›åº¦æ˜¾ç¤ºæœºåˆ¶
```python
class FFmpegProgressMonitor:
    """
    FFmpeg è¿›åº¦ç›‘æ§å™¨ - å®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦
    
    ä½¿ç”¨ -progress pipe:1 å‚æ•°è·å–FFmpegè¿›åº¦è¾“å‡ºï¼Œ
    è§£æ frame= è¡Œè·å–å·²å¤„ç†å¸§æ•°ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡ã€‚
    """
    
    def run_with_progress(self, cmd: List[str], total_frames: int, 
                         stage_name: str) -> bool:
        """
        è¿è¡ŒFFmpegå‘½ä»¤å¹¶æ˜¾ç¤ºå®æ—¶è¿›åº¦
        
        Args:
            cmd: FFmpegå‘½ä»¤åˆ—è¡¨
            total_frames: æ€»å¸§æ•°
            stage_name: é˜¶æ®µåç§° (ç”¨äºæ˜¾ç¤º)
        """
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            universal_newlines=True
        )
        
        frames_processed = 0
        last_print_frame = 0
        
        while True:
            line = process.stdout.readline()
            if not line and process.poll() is not None:
                break
            
            # è§£æè¿›åº¦è¾“å‡º (æ ¼å¼: frame=123)
            if line.startswith('frame='):
                frames_processed = int(line.strip().split('=')[1])
                
                if total_frames > 0:
                    progress = min(frames_processed / total_frames, 1.0)
                    
                    # æ¯100å¸§æ›´æ–°ä¸€æ¬¡æ˜¾ç¤º
                    if frames_processed - last_print_frame >= 100:
                        last_print_frame = frames_processed
                        bar_width = 40
                        filled = int(bar_width * progress)
                        bar = 'â–ˆ' * filled + 'â–‘' * (bar_width - filled)
                        print(f"\r   è¿›åº¦: [{bar}] {progress*100:.1f}% "
                              f"({frames_processed}/{total_frames}å¸§)", 
                              end='', flush=True)
        
        print()  # æ¢è¡Œ
        return process.returncode == 0
```

**è¿›åº¦æ˜¾ç¤ºç¤ºä¾‹è¾“å‡º**:
```
ğŸ¬ Stage 4.1 - ç»“æ„é‡å»º æ­¥éª¤ 1/2
   åˆ†è¾¨ç‡: 1920x1080 -> 3840x2160
   æ€»å¸§æ•°: 7500
   è¿›åº¦: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 50.0% (3750/7500å¸§)

ğŸ¨ Stage 4.2 - GANå¢å¼º (å¼ºåº¦: medium)
   æ€»å¸§æ•°: 7500
   è¿›åº¦: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100.0% (7500/7500å¸§)

ğŸ”’ Stage 4.3 - æ—¶åºé”å®š + éŸ³é¢‘åˆå¹¶
   æ€»å¸§æ•°: 7500
   éŸ³é¢‘: æœ‰
   è¿›åº¦: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100.0% (7500/7500å¸§)
```

#### åŠ¨æ€å¹¶è¡Œå¤„ç†ä¼˜åŒ–
```python
import multiprocessing
from concurrent.futures import ThreadPoolExecutor
from typing import List, Callable

class DynamicParallelProcessingOptimizer:
    """
    åŠ¨æ€å¹¶è¡Œå¤„ç†ä¼˜åŒ–å™¨ - æ ¹æ®ç³»ç»Ÿèµ„æºè‡ªé€‚åº”è°ƒæ•´
    """
    
    def __init__(self):
        # åŠ¨æ€æ£€æµ‹CPUæ ¸å¿ƒæ•°
        self.cpu_cores = multiprocessing.cpu_count()
        self.max_workers = min(8, self.cpu_cores)  # æœ€å¤š8ä¸ªå·¥ä½œçº¿ç¨‹
        
        # GPUå¤„ç†ä¿æŒä¸²è¡Œ
        self.gpu_serial_lock = threading.Lock()
        
        logger.info(f"Detected {self.cpu_cores} CPU cores, using {self.max_workers} workers")
    
    def optimize_cpu_processing(self, items: List[Any], 
                              process_func: Callable,
                              progress_callback: Optional[Callable] = None) -> List[Any]:
        """
        CPUé˜¶æ®µå¹¶è¡Œå¤„ç†ä¼˜åŒ–
        """
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_item = {
                executor.submit(process_func, item): item 
                for item in items
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in concurrent.futures.as_completed(future_to_item):
                try:
                    result = future.result()
                    results.append(result)
                    
                    completed += 1
                    if progress_callback:
                        progress_callback(completed / len(items))
                        
                except Exception as e:
                    logger.error(f"Processing failed for item: {e}")
                    results.append(None)
        
        return results
    
    def optimize_gpu_processing(self, items: List[Any],
                              process_func: Callable,
                              progress_callback: Optional[Callable] = None) -> List[Any]:
        """
        GPUé˜¶æ®µä¸²è¡Œå¤„ç†ï¼ˆé¿å…æ˜¾å­˜å†²çªï¼‰
        """
        results = []
        
        with self.gpu_serial_lock:
            for i, item in enumerate(items):
                try:
                    result = process_func(item)
                    results.append(result)
                    
                    if progress_callback:
                        progress_callback((i + 1) / len(items))
                        
                except Exception as e:
                    logger.error(f"GPU processing failed for item {i}: {e}")
                    results.append(None)
        
        return results
    
    def optimize_mixed_processing(self, frames: List[np.ndarray],
                                cpu_preprocess: Callable,
                                gpu_process: Callable,
                                cpu_postprocess: Callable,
                                progress_callback: Optional[Callable] = None) -> List[np.ndarray]:
        """
        æ··åˆå¤„ç†ä¼˜åŒ–ï¼šCPUé¢„å¤„ç† -> GPUå¤„ç† -> CPUåå¤„ç†
        """
        # 1. CPUå¹¶è¡Œé¢„å¤„ç†
        preprocessed_frames = self.optimize_cpu_processing(
            frames, cpu_preprocess,
            lambda p: progress_callback(p * 0.3) if progress_callback else None
        )
        
        # 2. GPUä¸²è¡Œå¤„ç†
        gpu_processed_frames = self.optimize_gpu_processing(
            preprocessed_frames, gpu_process,
            lambda p: progress_callback(0.3 + p * 0.4) if progress_callback else None
        )
        
        # 3. CPUå¹¶è¡Œåå¤„ç†
        final_frames = self.optimize_cpu_processing(
            gpu_processed_frames, cpu_postprocess,
            lambda p: progress_callback(0.7 + p * 0.3) if progress_callback else None
        )
        
        return final_frames
```

### å†…å­˜ç®¡ç†ç­–ç•¥

#### 6GBæ˜¾å­˜çº¦æŸä¸‹çš„ç²¾ç¡®å†…å­˜ç®¡ç†
```python
class PreciseMemoryManager:
    """
    ç²¾ç¡®çš„GPUå†…å­˜ç®¡ç†å™¨
    """
    
    def __init__(self, max_gpu_memory_mb: int = 5500):
        self.max_gpu_memory_mb = max_gpu_memory_mb
        self.current_usage_mb = 0
        self.memory_reservations = {}
        
    def check_gpu_memory_available(self) -> int:
        """
        æ£€æŸ¥å¯ç”¨GPUå†…å­˜ï¼ˆMBï¼‰
        """
        if not torch.cuda.is_available():
            return 0
        
        # è·å–å®é™…GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        total_memory = torch.cuda.get_device_properties(0).total_memory
        allocated_memory = torch.cuda.memory_allocated(0)
        cached_memory = torch.cuda.memory_reserved(0)
        
        used_memory_mb = (allocated_memory + cached_memory) // (1024 * 1024)
        available_mb = (total_memory // (1024 * 1024)) - used_memory_mb
        
        # åº”ç”¨å®‰å…¨é™åˆ¶
        safe_available = min(available_mb, self.max_gpu_memory_mb - used_memory_mb)
        
        return max(0, safe_available)
    
    def reserve_memory(self, operation_id: str, required_mb: int) -> bool:
        """
        é¢„ç•™å†…å­˜ç”¨äºç‰¹å®šæ“ä½œ
        """
        available = self.check_gpu_memory_available()
        
        if available >= required_mb:
            self.memory_reservations[operation_id] = required_mb
            return True
        else:
            logger.warning(f"Insufficient GPU memory: need {required_mb}MB, available {available}MB")
            return False
    
    def release_memory(self, operation_id: str) -> None:
        """
        é‡Šæ”¾é¢„ç•™çš„å†…å­˜
        """
        if operation_id in self.memory_reservations:
            del self.memory_reservations[operation_id]
            
            # å¼ºåˆ¶æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
    
    def get_optimal_tile_size(self, base_tile_size: int, model_memory_mb: int) -> int:
        """
        æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—æœ€ä¼˜ç“¦ç‰‡å¤§å°
        """
        available = self.check_gpu_memory_available()
        
        if available < model_memory_mb:
            return 64  # æœ€å°ç“¦ç‰‡å¤§å°
        
        # æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´ç“¦ç‰‡å¤§å°
        memory_ratio = (available - model_memory_mb) / 1000  # æ¯1GBé¢å¤–å†…å­˜
        
        if memory_ratio >= 3:
            return min(base_tile_size * 2, 512)  # å¤§ç“¦ç‰‡
        elif memory_ratio >= 1:
            return base_tile_size  # æ ‡å‡†ç“¦ç‰‡
        else:
            return max(base_tile_size // 2, 64)  # å°ç“¦ç‰‡
```

### è´¨é‡ä¿è¯è®¾è®¡

#### æ¯ç‰ˆçº§è´¨é‡éªŒè¯å™¨
```python
class MasterGradeQualityValidator:
    """
    æ¯ç‰ˆçº§è´¨é‡éªŒè¯å™¨ - å…·ä½“æŒ‡æ ‡å®ç°
    """
    
    def validate_master_quality(self, input_path: str, 
                               output_path: str) -> QualityReport:
        """
        æ‰§è¡Œæ¯ç‰ˆçº§è´¨é‡éªŒè¯
        """
        report = QualityReport()
        
        # 1. è§†é¢‘è´¨é‡éªŒè¯
        report.video_quality = self._validate_video_quality(output_path)
        
        # 2. éŸ³é¢‘è´¨é‡éªŒè¯
        report.audio_quality = self._validate_audio_quality(output_path)
        
        # 3. åŒæ­¥æ€§éªŒè¯
        report.sync_quality = self._validate_av_sync(output_path)
        
        # 4. æŠ€æœ¯è§„æ ¼éªŒè¯
        report.technical_specs = self._validate_technical_specs(output_path)
        
        return report
    
    def _validate_video_quality(self, video_path: str) -> VideoQualityMetrics:
        """
        è§†é¢‘è´¨é‡éªŒè¯æŒ‡æ ‡
        """
        cap = cv2.VideoCapture(video_path)
        
        brightness_values = []
        edge_stability_scores = []
        highlight_clipping_ratios = []
        
        previous_frame = None
        frame_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # 1. äº®åº¦ç¨³å®šæ€§æ£€æµ‹
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            brightness = np.mean(gray) / 255.0
            brightness_values.append(brightness)
            
            # 2. è¾¹ç¼˜ç¨³å®šæ€§è¯„ä¼°
            if previous_frame is not None:
                prev_edges = cv2.Canny(cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY), 50, 150)
                curr_edges = cv2.Canny(gray, 50, 150)
                
                # è®¡ç®—è¾¹ç¼˜ä¸€è‡´æ€§
                edge_diff = cv2.absdiff(prev_edges, curr_edges)
                stability_score = 1.0 - (np.sum(edge_diff > 0) / (frame.shape[0] * frame.shape[1]))
                edge_stability_scores.append(stability_score)
            
            # 3. é«˜å…‰æº¢å‡ºæ£€æµ‹
            highlight_mask = np.any(frame > 250, axis=2)  # æ¥è¿‘255çš„åƒç´ 
            highlight_ratio = np.sum(highlight_mask) / (frame.shape[0] * frame.shape[1])
            highlight_clipping_ratios.append(highlight_ratio)
            
            previous_frame = frame
            
            # é‡‡æ ·å¤„ç†ï¼Œé¿å…å¤„ç†æ‰€æœ‰å¸§
            if frame_count >= 100:  # æœ€å¤šæ£€æŸ¥100å¸§
                break
        
        cap.release()
        
        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        brightness_stability = 1.0 - np.std(brightness_values)  # äº®åº¦ç¨³å®šæ€§
        edge_stability = np.mean(edge_stability_scores) if edge_stability_scores else 1.0
        highlight_clipping = np.mean(highlight_clipping_ratios)
        
        return VideoQualityMetrics(
            brightness_stability=max(0, brightness_stability),
            edge_stability=edge_stability,
            highlight_clipping=highlight_clipping,
            temporal_consistency=edge_stability,  # ä½¿ç”¨è¾¹ç¼˜ç¨³å®šæ€§ä½œä¸ºæ—¶åºä¸€è‡´æ€§æŒ‡æ ‡
            resolution_accuracy=self._verify_resolution_accuracy(video_path)
        )
    
    def _validate_audio_quality(self, video_path: str) -> AudioQualityMetrics:
        """
        éŸ³é¢‘è´¨é‡éªŒè¯
        """
        # æå–éŸ³é¢‘è¿›è¡Œåˆ†æ
        temp_audio = "/tmp/temp_audio_analysis.wav"
        subprocess.run([
            'ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', 
            '-ar', '44100', '-y', temp_audio
        ], capture_output=True)
        
        # åŠ è½½éŸ³é¢‘æ•°æ®
        y, sr = librosa.load(temp_audio, sr=44100)
        
        # 1. å¯¹ç™½æ¸…æ™°åº¦è¯„åˆ†ï¼ˆåŸºäºé¢‘è°±ç‰¹å¾ï¼‰
        # äººå£°é¢‘ç‡èŒƒå›´å¤§è‡´åœ¨300-3400Hz
        stft = librosa.stft(y)
        freqs = librosa.fft_frequencies(sr=sr)
        
        # è®¡ç®—äººå£°é¢‘æ®µçš„èƒ½é‡æ¯”ä¾‹
        voice_freq_mask = (freqs >= 300) & (freqs <= 3400)
        voice_energy = np.mean(np.abs(stft[voice_freq_mask, :]))
        total_energy = np.mean(np.abs(stft))
        
        dialogue_clarity = voice_energy / total_energy if total_energy > 0 else 0
        
        # 2. åŠ¨æ€èŒƒå›´è®¡ç®—
        rms_energy = librosa.feature.rms(y=y)[0]
        dynamic_range = np.max(rms_energy) / (np.mean(rms_energy) + 1e-8)
        
        # 3. å“åº¦æµ‹é‡ï¼ˆç®€åŒ–ç‰ˆLUFSï¼‰
        # å®é™…åº”ä½¿ç”¨ä¸“ä¸šçš„å“åº¦æµ‹é‡åº“
        loudness_lufs = 20 * np.log10(np.sqrt(np.mean(y**2)) + 1e-8)
        
        # 4. éŸ³é‡ä¸€è‡´æ€§
        # è®¡ç®—RMSèƒ½é‡çš„å˜å¼‚ç³»æ•°
        volume_consistency = 1.0 - (np.std(rms_energy) / (np.mean(rms_energy) + 1e-8))
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_audio)
        
        return AudioQualityMetrics(
            dialogue_clarity_score=min(1.0, dialogue_clarity * 2),  # å½’ä¸€åŒ–åˆ°0-1
            dynamic_range=min(10.0, dynamic_range),  # é™åˆ¶åœ¨åˆç†èŒƒå›´
            loudness_lufs=loudness_lufs,
            volume_consistency=max(0, volume_consistency)
        )

@dataclass
class VideoQualityMetrics:
    brightness_stability: float    # äº®åº¦ç¨³å®šæ€§ (0-1)
    edge_stability: float         # è¾¹ç¼˜ç¨³å®šæ€§ (0-1)
    highlight_clipping: float     # é«˜å…‰æº¢å‡ºæ¯”ä¾‹ (0-1)
    temporal_consistency: float   # æ—¶åºä¸€è‡´æ€§ (0-1)
    resolution_accuracy: float    # åˆ†è¾¨ç‡å‡†ç¡®æ€§ (0-1)

@dataclass
class AudioQualityMetrics:
    dialogue_clarity_score: float  # å¯¹ç™½æ¸…æ™°åº¦ (0-1)
    dynamic_range: float          # åŠ¨æ€èŒƒå›´ (dB)
    loudness_lufs: float          # å“åº¦ (LUFS)
    volume_consistency: float     # éŸ³é‡ä¸€è‡´æ€§ (0-1)

@dataclass
class QualityReport:
    video_quality: VideoQualityMetrics
    audio_quality: AudioQualityMetrics
    sync_quality: float           # éŸ³è§†é¢‘åŒæ­¥è´¨é‡ (0-1)
    technical_specs: Dict[str, Any]  # æŠ€æœ¯è§„æ ¼éªŒè¯ç»“æœ
```

## é…ç½®ç®¡ç†è®¾è®¡

### ç­–ç•¥é…ç½®æ¨¡æ¿ï¼ˆæ›´æ–°ç‰ˆï¼‰
```json
{
  "theater_enhancement_config": {
    "version": "2.0.0",
    "presets": {
      "theater_small": {
        "lighting_sensitivity": 0.8,
        "gan_policy": {
          "strength": "weak",
          "highlight_threshold": 0.85,
          "shadow_threshold": 0.15,
          "edge_threshold": 0.1,
          "motion_threshold": 0.05
        },
        "temporal_config": {
          "background_lock": true,
          "strength": "high",
          "optical_flow_enabled": true,
          "smoothing_alpha": 0.1
        },
        "audio_config": {
          "source_separation_enabled": true,
          "dialogue_enhancement": 0.8,
          "music_processing": "preserve",
          "ambient_processing": "spatial"
        }
      },
      "theater_medium": {
        "lighting_sensitivity": 1.0,
        "gan_policy": {
          "strength": "medium",
          "highlight_threshold": 0.80,
          "shadow_threshold": 0.20,
          "edge_threshold": 0.08,
          "motion_threshold": 0.03
        },
        "temporal_config": {
          "background_lock": true,
          "strength": "medium",
          "optical_flow_enabled": true,
          "smoothing_alpha": 0.3
        },
        "audio_config": {
          "source_separation_enabled": true,
          "dialogue_enhancement": 0.6,
          "music_processing": "enhance",
          "ambient_processing": "spatial"
        }
      },
      "theater_large": {
        "lighting_sensitivity": 1.2,
        "gan_policy": {
          "strength": "strong",
          "highlight_threshold": 0.75,
          "shadow_threshold": 0.25,
          "edge_threshold": 0.06,
          "motion_threshold": 0.02
        },
        "temporal_config": {
          "background_lock": false,
          "strength": "low",
          "optical_flow_enabled": false,
          "smoothing_alpha": 0.5
        },
        "audio_config": {
          "source_separation_enabled": true,
          "dialogue_enhancement": 0.4,
          "music_processing": "enhance",
          "ambient_processing": "preserve"
        }
      }
    },
    "hardware_profiles": {
      "gtx_1650_4gb": {
        "max_tile_size": 128,
        "max_batch_size": 1,
        "force_fp16": true,
        "max_models_loaded": 1,
        "max_workers": 4
      },
      "rtx_3060_12gb": {
        "max_tile_size": 256,
        "max_batch_size": 4,
        "force_fp16": false,
        "max_models_loaded": 2,
        "max_workers": 6
      }
    },
    "audio_dependencies": {
      "required_packages": [
        "spleeter>=2.3.0",
        "pydub>=0.25.1",
        "librosa>=0.9.0",
        "noisereduce>=2.0.0"
      ],
      "fallback_enabled": true
    }
  }
}
```

è¿™ä¸ªä¼˜åŒ–ç‰ˆçš„è®¾è®¡æ–‡æ¡£ç°åœ¨åŒ…å«äº†ï¼š

1. **å¤šç»´åº¦GANå®‰å…¨åŒºåŸŸMaskç”Ÿæˆ** - å…·ä½“çš„ç®—æ³•å®ç°
2. **å…‰æµå¢å¼ºçš„æ—¶åºé”å®šå¤„ç†** - å®Œæ•´çš„å…‰æµå’Œå¸§é—´å¹³æ»‘ç®—æ³•
3. **åŠ¨æ€å¹¶è¡Œå¤„ç†ä¼˜åŒ–** - è‡ªé€‚åº”CPUæ ¸å¿ƒæ£€æµ‹å’Œæ··åˆå¤„ç†ç­–ç•¥
4. **å…·ä½“åº“è°ƒç”¨çš„éŸ³é¢‘å¤„ç†** - FFmpegã€Spleeterã€pydubç­‰å…·ä½“å®ç°
5. **ç²¾ç¡®çš„å†…å­˜ç®¡ç†** - 6GBæ˜¾å­˜çº¦æŸä¸‹çš„ç²¾ç¡®æ§åˆ¶
6. **æ¯ç‰ˆçº§è´¨é‡éªŒè¯** - å…·ä½“çš„è´¨é‡æŒ‡æ ‡è®¡ç®—ç®—æ³•

ç°åœ¨è®©æˆ‘æ›´æ–°tasks.mdæ–‡æ¡£ä»¥åæ˜ è¿™äº›ä¼˜åŒ–ï¼š